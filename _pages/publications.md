---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

- **Accelerating Transformer Pre-training with 2:4 Sparsity** [[arXiv]](https://arxiv.org/abs/2404.01847) [[OpenReview]](https://openreview.net/forum?id=kTaX87Zn6M) [[PDF]](https://proceedings.mlr.press/v235/hu24r.html) [[Project page]](https://github.com/huyz2023/2by4-pretrain)

  **Yuezhou Hu**, Kang Zhao, Weiyu Huang, Jianfei Chen, Jun Zhu

  International Conference on Machine Learning (ICML), 2024

- **S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training** [[arXiv]](https://arxiv.org/abs/2409.09099) [[OpenReview]](https://openreview.net/forum?id=8abNCVJs2j) [[Project page]](https://github.com/huyz2023/2by4-pretrain)

  **Yuezhou Hu**, Jun Zhu, Jianfei Chen

  Neural Information Processing Systems (NeurIPS), 2024

# Working Papers

- **Pruning Large Language Models with Semi-Structural Adaptive Sparse Training** [[arXiv]](https://arxiv.org/abs/2407.20584)

  Weiyu Huang, **Yuezhou Hu**, Guohao Jian, Jun Zhu, Jianfei Chen
